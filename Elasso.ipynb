{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = np.random.random(100*9)\n",
    "X_raw = np.reshape(X_raw, (100,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add an intercept column to the model\n",
    "X = np.abs(np.concatenate((np.ones((X.shape[0], 1)), X), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([0.5,0.6,0.7,0.3,0.5,0.7,0.1,0.2,0.2,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.matmul(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elasso:\n",
    "    \"\"\"\n",
    "    This solves the customized linear model I proposed.\n",
    "    \"\"\"\n",
    "    def __init__(self,X, y, lambda1=1, lambda2=1, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Coordiante descent algorithm (cyclic) to solve the customized linear model.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: matrix (n by k)\n",
    "        The exogenous variables for Elasso.\n",
    "        \n",
    "        y: vector (n by 1)\n",
    "        The endogenoud variable for Elasso.\n",
    "        \n",
    "        lambda_1: float\n",
    "        Equality regularization parameter.\n",
    "        The default lambda_1 is set to 1. However, users are allowed to set lambda_1 in the __main__ function \n",
    "        to see different training performances.\n",
    "        \n",
    "        lambda_2: float\n",
    "        Sparsity regularization parameter.\n",
    "        The default lambda_2 is set to 1. However, users are allowed to set lambda_2 in the __main__function.\n",
    "        \n",
    "        max_iter: int\n",
    "        Default is set as 1000.\n",
    "        However, users are allowed to set max_iter in the __main__ function to see different performances.\n",
    "        \n",
    "        beta: vector\n",
    "        This is the estimated beta computed from the coordinate descent algorithm using the brute force \n",
    "        way though.\n",
    "        \"\"\"\n",
    "        #Validate inputs\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The input size does not match\")\n",
    "        # Initialize class instance variables:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.max_iter = max_iter\n",
    "        self.beta = self.coordinatedescent()\n",
    "        \n",
    "    def customizedloss(self, beta):\n",
    "        \"\"\"\n",
    "        This is a customized loss function for the customized LASSO function in my research, where lambda1 is the hyperparameter for\n",
    "        the equality constraint where lambda2 is the constraint for sparsity.\n",
    "        \"\"\"\n",
    "        penalty1 = 0\n",
    "        for i in range(len(beta)-1):\n",
    "            penalty1 += abs(beta[i] - beta[i+1])\n",
    "        penalty1 = penalty1*self.lambda1\n",
    "        penalty2 = sum([abs(b) for b in beta])\n",
    "        penalty2 = self.lambda2*penalty2\n",
    "        #print(\"penalty2 is %s\" %penalty2)\n",
    "        y_true = np.array(self.y)\n",
    "        y_pred = np.array(np.matmul(self.X, beta))\n",
    "        assert len(y_true) == len(y_pred)\n",
    "        fittingloss = np.sum((y_true - y_pred)**2)\n",
    "        totalloss = fittingloss + penalty1 + penalty2\n",
    "        return totalloss\n",
    "    \n",
    "    def coordinatedescent(self):\n",
    "        # Initialization \n",
    "        intbeta = [0] * self.X.shape[1]\n",
    "        # This step I split the \n",
    "        Range = np.linspace(-0.9999, 0.9999, 1000)\n",
    "        num = 0\n",
    "        oldbeta = intbeta.copy()\n",
    "        newbeta = intbeta.copy()\n",
    "        while num < self.max_iter:\n",
    "            for i in range(len(intbeta)):\n",
    "                loss = []\n",
    "                #print(\"This is the original new beta, %s\" % newbeta)\n",
    "                for x in Range:\n",
    "                    betatemp = newbeta.copy()\n",
    "                    betatemp[i] = x\n",
    "                    value = self.customizedloss(betatemp)\n",
    "                    loss.append(value)\n",
    "                minIndex = loss.index(min(loss))\n",
    "                newbeta[i] = Range[minIndex]\n",
    "                #print(\"This is the new beta after minimization, %s\" % newbeta)\n",
    "            mse = calmse(oldbeta, newbeta)\n",
    "            #print(\"This is the old beta before substitution, %s\" % oldbeta)\n",
    "            oldbeta = newbeta.copy()\n",
    "            #print(\"This is the old beta after substitution, %s\" % oldbeta)\n",
    "            num += 1\n",
    "            print(\"This is the mse, %s\" %mse)\n",
    "            if mse < 0.00000001:\n",
    "                return newbeta\n",
    "                break\n",
    "                print(\"Maximum iteration reached\")\n",
    "        return newbeta\n",
    "    \n",
    "def calmse(oldbeta, newbeta):\n",
    "    mse = 0\n",
    "    for old, new in zip(oldbeta, newbeta):\n",
    "        mse +=(old - new)**2\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mse, 4.334898084942781\n",
      "This is the mse, 0.10326982060222391\n",
      "This is the mse, 0.06564211444200964\n",
      "This is the mse, 0.04388696888888884\n",
      "This is the mse, 0.03134440016881747\n",
      "This is the mse, 0.024451998188458703\n",
      "This is the mse, 0.020152261371641954\n",
      "This is the mse, 0.017158875162730285\n",
      "This is the mse, 0.01471046957552147\n",
      "This is the mse, 0.012462424510997449\n",
      "This is the mse, 0.01032658133917704\n",
      "This is the mse, 0.008411134742309872\n",
      "This is the mse, 0.0064636304618131656\n",
      "This is the mse, 0.0048206741757974144\n",
      "This is the mse, 0.003570424514243971\n",
      "This is the mse, 0.002580643532180841\n",
      "This is the mse, 0.0017391293369044612\n",
      "This is the mse, 0.0013063506079052057\n",
      "This is the mse, 0.0009216584043502923\n",
      "This is the mse, 0.0007453411443876354\n",
      "This is the mse, 0.0007733916175635066\n",
      "This is the mse, 0.0006812257771284812\n",
      "This is the mse, 0.0008855935102670254\n",
      "This is the mse, 0.000805449301193088\n",
      "This is the mse, 0.0008855935102670231\n",
      "This is the mse, 0.0010018026134242346\n",
      "This is the mse, 0.0009817665611557473\n",
      "This is the mse, 0.001081946822498178\n",
      "This is the mse, 0.0009497088775261676\n",
      "This is the mse, 0.0008334997743689726\n",
      "This is the mse, 0.0008415141952763501\n",
      "This is the mse, 0.0008294925639152642\n",
      "This is the mse, 0.0007172906712117482\n",
      "This is the mse, 0.0006932474084895808\n",
      "This is the mse, 0.0005970743576008475\n",
      "This is the mse, 0.0005009013067121062\n",
      "This is the mse, 0.0004808652544436383\n",
      "This is the mse, 0.00039671383491599705\n",
      "This is the mse, 0.00030454799448096406\n",
      "This is the mse, 0.0002444398376755186\n",
      "This is the mse, 0.0002404326272218129\n",
      "This is the mse, 0.00018833889132375748\n",
      "This is the mse, 0.00017631725996266446\n",
      "This is the mse, 0.00018032447041636034\n",
      "This is the mse, 0.00017631725996266535\n",
      "This is the mse, 0.0001202163136109078\n",
      "This is the mse, 9.216584043502945e-05\n",
      "This is the mse, 0.00012021631361090958\n",
      "This is the mse, 8.01442090739418e-05\n",
      "This is the mse, 5.209373589805812e-05\n",
      "This is the mse, 8.014420907393912e-05\n",
      "This is the mse, 4.808652544436134e-05\n",
      "This is the mse, 5.209373589806167e-05\n",
      "This is the mse, 4.8086525444363116e-05\n",
      "This is the mse, 4.808652544436489e-05\n",
      "This is the mse, 4.8086525444364004e-05\n",
      "This is the mse, 2.404326272218067e-05\n",
      "This is the mse, 2.8050473175877448e-05\n",
      "This is the mse, 2.4043262722182446e-05\n",
      "This is the mse, 2.404326272218067e-05\n",
      "This is the mse, 2.404326272218245e-05\n",
      "This is the mse, 2.4043262722183337e-05\n",
      "This is the mse, 8.014420907393557e-06\n",
      "This is the mse, 8.014420907394447e-06\n",
      "This is the mse, 8.014420907393557e-06\n",
      "This is the mse, 8.014420907393557e-06\n",
      "This is the mse, 1.2021631361091225e-05\n",
      "This is the mse, 2.0036052268483893e-05\n",
      "This is the mse, 0.0\n"
     ]
    }
   ],
   "source": [
    "model = Elasso(X, Y, lambda1=0, lambda2=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.6, 0.7, 0.3, 0.5, 0.7, 0.1, 0.2, 0.2, 0.8])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(Y_true, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
